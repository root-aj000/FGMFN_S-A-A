This file defines a **PyTorch Dataset** that:

- Reads images and text from a CSV + image folder  
- Preprocesses them  
- Packs them into a format your model (`FG_MFN`) expects  
- Handles both old “single label” data and new “multi-attribute” data

I’ll walk through it in order, with a mental picture of what’s happening.

---

## 1. Imports

```python
import os
import cv2
import torch
from torch.utils.data import Dataset
import pandas as pd
import numpy as np
from preprocessing.image_preprocessing import resize_image, normalize_image
from preprocessing.text_preprocessing import clean_text, tokenize_text
from preprocessing.augmentation import augment_image
from utils.path import IMAGE_DIR, TRAIN_CSV
```

- `os` – for file paths (`os.path.join`).
- `cv2` – OpenCV, used to read and manipulate images.
- `torch` – PyTorch main library.
- `from torch.utils.data import Dataset` – base class for custom datasets in PyTorch.
- `pandas as pd` – for reading CSV files into DataFrames.
- `numpy as np` – for numerical arrays; used to create fallback images.
- `resize_image`, `normalize_image` – helper functions to make images the right size and format for the model.
- `clean_text`, `tokenize_text` – helper functions to clean raw text and turn it into token IDs and attention masks.
- `augment_image` – random image transformations (flip, rotate, crop).
- `IMAGE_DIR`, `TRAIN_CSV` – default paths defined elsewhere.

---

## 2. Config / Paths

```python
# Config / Paths
IMAGE_DIR = IMAGE_DIR
MAX_TEXT_LEN = 128  # Max length for tokenized text
```

- `IMAGE_DIR = IMAGE_DIR`  
  Just re‑assigns the imported global constant to a local variable with the same name. Functionally, it doesn’t change the value.

- `MAX_TEXT_LEN = 128`  
  Maximum length for tokenized text sequences.  
  Cutting or padding all texts to 128 tokens keeps them a fixed size, which models like BERT require.

Think:

> Every text → token IDs of length 128 (with truncation or padding as needed).

---

## 3. Attribute names

```python
# Attribute names that we train on (must match model config)
ATTRIBUTE_NAMES = [
    "theme", "sentiment", "emotion", "dominant_colour", "attention_score",
    "trust_safety", "target_audience", "predicted_ctr", "likelihood_shares"
]
```

This is the **list of attributes** your model might predict (same as in `FG_MFN`):

- `theme`
- `sentiment`
- `emotion`
- `dominant_colour`
- `attention_score`
- `trust_safety`
- `target_audience`
- `predicted_ctr`
- `likelihood_shares`

Each of these has a corresponding numeric column in the CSV (like `sentiment_num`, `emotion_num`, etc.) if it’s present.

---

## 4. The `CustomDataset` class

```python
class CustomDataset(Dataset):
```

- This class implements a **PyTorch Dataset**:  
  it defines how to:

  - Count examples (`__len__`)
  - Fetch an example by index (`__getitem__`)

PyTorch’s `DataLoader` will use this to create batches for training.

---

## 5. Constructor: `__init__`

```python
    def __init__(self, csv_path, image_dir=IMAGE_DIR, augment=False):
        """
        Args:
            csv_path (str): Path to CSV file with columns: image_path, text, and attribute columns
            image_dir (str): Directory containing images
            augment (bool): Apply image augmentation
        """
```

- Parameters:
  - `csv_path`: where the CSV file is (e.g., `train.csv`).
  - `image_dir`: directory that contains the image files.
  - `augment`: whether to apply random image augmentation (True for training, False for val/test).

---

### 5.1 Load CSV and basic setup

```python
        self.df = pd.read_csv(csv_path)
        self.image_dir = image_dir
        self.augment = augment
```

- `self.df = pd.read_csv(csv_path)`  
  Loads the CSV into a pandas DataFrame `self.df`.  
  Each row corresponds to one data sample.

  Typical columns:
  - `image_path` – relative path or filename of the image.
  - `text` – associated text.
  - `..._num` – numeric labels for each attribute.

- `self.image_dir` – store image directory.
- `self.augment` – store whether to apply augmentation.

---

### 5.2 Determine available attributes in this CSV

```python
        # Determine which attributes are present in the CSV
        self.available_attributes = []
        for attr in ATTRIBUTE_NAMES:
            col_name = f"{attr}_num"
            if col_name in self.df.columns:
                self.available_attributes.append(attr)
```

- Initializes `self.available_attributes` as an empty list.

Then, for each attribute name in `ATTRIBUTE_NAMES`:
- It checks if the corresponding column `"{attr}_num"` exists in the CSV.

Example:
- For `attr = "sentiment"`, it checks for `"sentiment_num"` column.
- If present, adds `"sentiment"` to `self.available_attributes`.

Purpose:

> Different CSVs might have different sets of attributes. Here we automatically detect which ones are actually present instead of assuming all are there.

---

### 5.3 Legacy mode detection (backwards compatibility)

```python
        # Backwards compatibility: check for old label_num column
        self.legacy_mode = "label_num" in self.df.columns and len(self.available_attributes) == 0
```

- This handles older CSV formats that didn’t have per-attribute columns, but just one column `label_num`.

`legacy_mode` is True if:
- The CSV has a `"label_num"` column  
**and**
- It does **not** have any attribute-specific columns (no `sentiment_num`, `emotion_num`, etc.).

Meaning:

> In legacy mode, the dataset will provide a single label `"label"` instead of multiple attribute labels.

This lets old data still work with the new code.

---

## 6. `__len__`: how many samples are there?

```python
    def __len__(self):
        return len(self.df)
```

- Returns the number of rows in the DataFrame.
- This is how PyTorch knows the dataset size.

---

## 7. `__getitem__`: fetch one sample

```python
    def __getitem__(self, idx):
        row = self.df.iloc[idx]
```

- `idx` is the index of the row/sample.
- `row = self.df.iloc[idx]` gets that row as a Series-like object.

From here, we:
1. Load and preprocess the image.
2. Preprocess the text.
3. Attach labels.
4. Return everything in a dictionary.

---

### 7.1 Load and preprocess image

```python
        # --- Load and preprocess image ---
        image_path = os.path.join(self.image_dir, row["image_path"])
        img = cv2.imread(image_path)
        if img is None:
            # Return a black image if file missing/corrupted
            img = np.zeros((224, 224, 3), dtype=np.uint8)
        else:
            img = resize_image(img)
            # Convert BGR to RGB
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            if self.augment:
                img = augment_image(img)
        img_tensor = normalize_image(img)
```

Step by step:

1. **Construct full image path**
   ```python
   image_path = os.path.join(self.image_dir, row["image_path"])
   ```
   - Combines directory and filename from CSV.
   - E.g.: `"/data/images" + "img_001.jpg"` → `"/data/images/img_001.jpg"`

2. **Read image**
   ```python
   img = cv2.imread(image_path)
   ```
   - Attempts to load the image as a NumPy array (H, W, 3) in BGR format (OpenCV default).

3. **Handle missing image**
   ```python
   if img is None:
       img = np.zeros((224, 224, 3), dtype=np.uint8)
   ```
   - If the image file can’t be read (missing, corrupted), `cv2.imread` returns `None`.
   - In that case, they create a **black image** of size 224×224 with 3 channels.
   - This is a safe fallback so the code doesn’t crash.

4. **Else, normal processing**
   ```python
   else:
       img = resize_image(img)
       # Convert BGR to RGB
       img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
       if self.augment:
           img = augment_image(img)
   ```
   - `resize_image(img)`  
     Resizes the image to a consistent size (e.g., 224×224).
   - `cv2.cvtColor(img, cv2.COLOR_BGR2RGB)`  
     Converts from BGR (OpenCV convention) to RGB (standard for most models).
   - If `self.augment` is `True`, apply random augmentations:
     - Horizontal flip (maybe)
     - Random small rotation
     - Random crop + resize

5. **Normalize and convert to tensor**
   ```python
   img_tensor = normalize_image(img)
   ```
   - `normalize_image` likely:
     - Transposes to channels‑first: `[H, W, C]` → `[C, H, W]`.
     - Scales and normalizes pixel values.
     - Converts to a `torch.Tensor`.

So at the end:

- `img_tensor` is a model-ready image tensor, shape `[C, H, W]` (e.g., `[3, 224, 224]`).

---

### 7.2 Preprocess text

```python
        # --- Preprocess text ---
        text = clean_text(row.get("text", ""))
        text_tensor = tokenize_text(text, max_length=MAX_TEXT_LEN)
```

1. **Get text safely**
   ```python
   text = clean_text(row.get("text", ""))
   ```
   - `row.get("text", "")`:
     - Get the `"text"` field from the row, or use an empty string if it’s missing.
   - `clean_text(...)`:
     - Cleans the text (lowercasing, stripping whitespace, removing noise, etc.).

2. **Tokenize**
   ```python
   text_tensor = tokenize_text(text, max_length=MAX_TEXT_LEN)
   ```
   - Converts the cleaned string into:
     - `input_ids`: numeric token IDs.
     - `attention_mask`: which positions are real tokens vs padding.
   - Both truncated/padded to length `MAX_TEXT_LEN` (128).

`tokenize_text` here is assumed to return a dictionary like:

```python
{
    "input_ids": <tensor or array of shape [128]>,
    "attention_mask": <tensor or array of shape [128]>
}
```

---

### 7.3 Build the basic output dict (inputs)

```python
        # Build output dict
        output = {
            "visual": img_tensor,        # [C,H,W]
            "text": text_tensor["input_ids"],         # [seq_len]
            "attention_mask": text_tensor["attention_mask"], # [seq_len]
        }
```

- Creates a dictionary `output` to hold everything this sample needs.

Keys:

- `"visual"`: the image tensor `[C, H, W]`.
- `"text"`: the token IDs `[seq_len]` (here `seq_len = 128`).
- `"attention_mask"`: mask `[seq_len]` with 1s for real tokens and 0s for padding.

This format **matches the expectations of your model**:  
- `FG_MFN.forward(image_tensor, text_tensor, attention_mask=...)`.

---

### 7.4 Attach labels (either legacy single label or multi-attribute)

```python
        # --- Process labels ---
        if self.legacy_mode:
            # Backwards compatibility: single label
            output["label"] = torch.tensor(row.get("label_num", 0), dtype=torch.long)
        else:
            # Multi-attribute labels
            for attr in ATTRIBUTE_NAMES:
                col_name = f"{attr}_num"
                if col_name in self.df.columns:
                    output[attr] = torch.tensor(row.get(col_name, 0), dtype=torch.long)
                else:
                    # Default to 0 if attribute not in CSV
                    output[attr] = torch.tensor(0, dtype=torch.long)
```

Two possible modes:

#### a) Legacy mode (old data format)

- If `self.legacy_mode` is True:
  ```python
  output["label"] = torch.tensor(row.get("label_num", 0), dtype=torch.long)
  ```
  - Takes the `"label_num"` column from the row, or 0 if missing.
  - Wraps it in a PyTorch tensor of type `long` (integer class index).
  - Only one label field: `"label"`.

This is like a classic single-task classification dataset.

#### b) Multi-attribute mode (new format)

- Otherwise:
  ```python
  for attr in ATTRIBUTE_NAMES:
      col_name = f"{attr}_num"
      if col_name in self.df.columns:
          output[attr] = torch.tensor(row.get(col_name, 0), dtype=torch.long)
      else:
          # Default to 0 if attribute not in CSV
          output[attr] = torch.tensor(0, dtype=torch.long)
  ```
  - Loops over all `ATTRIBUTE_NAMES` (`"sentiment"`, `"emotion"`, etc.).
  - For each attribute:
    - Build the column name, e.g. `"sentiment_num"`.
    - If that column exists in the DataFrame:
      - Get its value from the current row (e.g. sentiment class index).
      - Convert to a `torch.long` tensor and store under `output[attr]`.  
        Example: `output["sentiment"] = tensor(2)`.
    - If the column does **not** exist:
      - Default label is 0 for that attribute.

Result:  
`output` now contains:

- Inputs:
  - `"visual"`
  - `"text"`
  - `"attention_mask"`
- And label(s):
  - Either `"label"` (legacy)  
  - Or multiple keys like `"sentiment"`, `"emotion"`, etc.

This lines up with the `FG_MFN` model, which returns a dict of attribute logits and expects you to match labels per attribute.

---

### 7.5 Return the sample

```python
        return output
```

- `output` is a dictionary structured roughly like:

Legacy case example:

```python
{
  "visual": <torch.Size([3, 224, 224])>,
  "text": <torch.Size([128])>,
  "attention_mask": <torch.Size([128])>,
  "label": <tensor(int)>
}
```

Multi-attribute case example:

```python
{
  "visual": <torch.Size([3, 224, 224])>,
  "text": <torch.Size([128])>,
  "attention_mask": <torch.Size([128])>,
  "sentiment": <tensor(int)>,
  "emotion": <tensor(int)>,
  "theme": <tensor(int)>,
  ...
}
```

The `DataLoader` will then stack these tensors into batches.

---

## 8. Example usage (when running this file directly)

```python
# Example usage
if __name__ == "__main__":
    train_csv = TRAIN_CSV
    dataset = CustomDataset(train_csv, augment=True)
    print(f"Dataset size: {len(dataset)}")
    print(f"Available attributes: {dataset.available_attributes}")
    print(f"Legacy mode: {dataset.legacy_mode}")

    sample = dataset[0]
    print("Sample keys:", sample.keys())
    print({k: v.shape if isinstance(v, torch.Tensor) else v for k, v in sample.items()})
```

This block is for quick testing / debugging.

- `if __name__ == "__main__":`  
  Means “run this code only if this file is executed directly (not imported).”

### 8.1 Create the dataset

```python
    train_csv = TRAIN_CSV
    dataset = CustomDataset(train_csv, augment=True)
```

- Uses the default training CSV path.
- Creates a `CustomDataset` in training mode (`augment=True`).

---

### 8.2 Print metadata

```python
    print(f"Dataset size: {len(dataset)}")
    print(f"Available attributes: {dataset.available_attributes}")
    print(f"Legacy mode: {dataset.legacy_mode}")
```

- Shows:
  - How many samples are in the dataset.
  - Which attributes it found in the CSV.
  - Whether it is using legacy mode or not.

Useful to verify the CSV is being read as expected.

---

### 8.3 Inspect a sample

```python
    sample = dataset[0]
    print("Sample keys:", sample.keys())
    print({k: v.shape if isinstance(v, torch.Tensor) else v for k, v in sample.items()})
```

- Fetches the first sample (`idx = 0`).
- Prints the keys of the returned dictionary.
- For any tensor values, prints their shapes instead of the full content.

Example output might be:

```text
Dataset size: 10000
Available attributes: ['sentiment', 'emotion']
Legacy mode: False
Sample keys: dict_keys(['visual', 'text', 'attention_mask', 'sentiment', 'emotion', 'theme', ...])
{'visual': torch.Size([3, 224, 224]),
 'text': torch.Size([128]),
 'attention_mask': torch.Size([128]),
 'sentiment': torch.Size([]),
 'emotion': torch.Size([]),
 ...}
```

This helps confirm that the dataset returns everything your model will need.

---

## Visual overview

Big picture of what `CustomDataset` does for each index `idx`:

```text
CSV row idx
   │
   ├─> image_path ──> read image (OpenCV) ──> resize ──> RGB ──> [augment?] ──> normalize ──> img_tensor [3,H,W]
   │
   ├─> text ──> clean_text ──> tokenize_text ──> input_ids [128], attention_mask [128]
   │
   └─> labels:
        • legacy: label_num → "label"
        • new: each attr_num column → "sentiment", "emotion", etc.

Return dict:
{
  "visual": img_tensor,
  "text": input_ids,
  "attention_mask": attention_mask,
  "sentiment": ...,
  "emotion": ...,
  ...
}
```

This fits neatly into your multimodal model, which expects image and text tensors plus attribute labels.

If you want, I can next show:

- How to plug this dataset into a `DataLoader` and loop over batches.
- Or how to write the training loop that uses `FG_MFN` with this `CustomDataset`.