This file defines **how raw text is cleaned and turned into token IDs** that BERT (and your `TextModule`) can understand.

I’ll go through it line by line, with simple and visual explanations.

---

## 1. Imports

```python
import re
from transformers import BertTokenizer
```

- `import re`  
  Imports Python’s **regular expression** module.  
  We use it for pattern-based text cleaning (removing unwanted characters, extra spaces, etc.).

- `from transformers import BertTokenizer`  
  Imports the **BERT tokenizer** from the Hugging Face `transformers` library.  
  The tokenizer converts normal text into:
  - `input_ids` (integers representing tokens)
  - `attention_mask` (which tokens are real vs padding)

---

## 2. Initialize the tokenizer

```python
# Initialize tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
```

- This line **loads a pretrained BERT tokenizer** by name: `"bert-base-uncased"`.

What it does conceptually:

- Downloads (or loads locally) a vocabulary and rules for:
  - Splitting text into subword tokens.
  - Mapping each token to an integer ID.
  - Adding special tokens like `[CLS]` and `[SEP]`.

“Uncased” means:

- It doesn’t distinguish between uppercase and lowercase:
  - `"Dog"` and `"dog"` are treated the same.

The `tokenizer` object is ready to be used to encode text into the format BERT expects.

---

## 3. `clean_text` function

```python
def clean_text(text):
    """Clean OCR text: lowercase, strip, remove special chars."""
```

This function prepares raw text by simplifying and cleaning it.  
The comment “Clean OCR text” suggests it might be used on text extracted from images (like via OCR), which can be noisy.

Let’s go through it line by line.

---

### 3.1 Handle non-string inputs

```python
    if not isinstance(text, str):
        return ""
```

- Checks if `text` is a string.
  - If not (e.g., `None`, a number, etc.), it returns an empty string `""`.

Reason:

> Prevent crashes if the CSV has missing text or weird data types.  
> If it’s not a string, we treat it as “no text”.

---

### 3.2 Lowercase and strip whitespace

```python
    text = text.lower().strip()
```

- `text.lower()`  
  Converts all characters to lowercase:
  - `"Hello WORLD!"` → `"hello world!"`

- `.strip()`  
  Removes leading and trailing whitespace:
  - `"  hello world  "` → `"hello world"`

Why?

- Lowercasing makes text **consistent** (BERT-uncased expects this).
- Stripping extra whitespace cleans up messy strings.

---

### 3.3 Remove special characters

```python
    text = re.sub(r"[^a-z0-9\s]", "", text)
```

- `re.sub(pattern, replacement, text)`  
  Replaces all parts of the text that match `pattern` with `replacement`.

Pattern: `r"[^a-z0-9\s]"`

- `^` inside `[]` means “not”.
- `a-z` = lowercase letters.
- `0-9` = digits.
- `\s` = whitespace (spaces, tabs, newlines).

So `[^a-z0-9\s]` means:

> Any character that is **not**:
> - a lowercase letter,
> - a digit,
> - or a whitespace character.

Those characters get replaced with `""` (empty string) → removed.

Examples:

- `"hello!!!"` → `"hello"`  
- `"price: $10.99"` → `"price 1099"` (removes `$` and `.`)
- `"Café & bar"` → `"caf  bar"` (removes `é` and `&`)

Purpose:

- Strip out punctuation, accents, symbols, and other special characters.
- Keep only letters, numbers, and spaces.

---

### 3.4 Collapse multiple spaces into one

```python
    text = re.sub(r"\s+", " ", text)
```

- Pattern: `r"\s+"`  
  - `\s` = whitespace.
  - `+` = one or more repetitions.

So `\s+` means “one or more whitespace characters in a row”.

- Replacing this with `" "` (single space) collapses sequences of spaces/tabs/newlines into a **single space**.

Example:

- `"hello     world"` → `"hello world"`
- `"hello \n\n world"` → `"hello world"`

---

### 3.5 Return the cleaned text

```python
    return text
```

At this point, `text` is:

- Lowercase
- Has no leading/trailing spaces
- Contains only:
  - lowercase letters `a-z`
  - digits `0-9`
  - single spaces
- Has no punctuation or special characters

This makes the text simpler and more uniform, which can help models generalize and reduce vocabulary noise.

---

## 4. `tokenize_text` function

```python
def tokenize_text(text, max_length=128):
    """Tokenize text using BERT tokenizer and return tensor of token ids."""
```

This function:

1. Takes a (cleaned) text string.
2. Uses the BERT tokenizer to convert it into:
   - `input_ids`
   - `attention_mask`
3. Ensures a fixed length (`max_length`), via padding and truncation.
4. Returns tensors ready for the `TextModule`.

---

### 4.1 Call the BERT tokenizer

```python
    encoding = tokenizer(
        text,
        add_special_tokens=True,
        max_length=max_length,
        padding="max_length",
        truncation=True,
        return_tensors="pt"
    )
```

This is the key step. Let’s break down each argument:

- `text`  
  The cleaned text string you want to tokenize.

- `add_special_tokens=True`  
  BERT uses special tokens like:
  - `[CLS]` at the start (classification token)
  - `[SEP]` at the end (separator)

  With this set to True, these tokens will be automatically added.

  Example token sequence might look like:

  ```text
  [CLS] this is some text [SEP]
  ```

- `max_length=max_length`  
  Maximum sequence length (default 128 here).
  - The output `input_ids` and `attention_mask` will have length exactly `max_length`.

- `padding="max_length"`  
  Tells the tokenizer to **pad** the sequence up to `max_length` using a special `[PAD]` token.

  Example:
  - If the tokenized sequence (with specials) has length 20, and `max_length` is 128:
    - It will add 108 `[PAD]` tokens at the end.

- `truncation=True`  
  If the tokenized text is longer than `max_length`, it will **cut off** (truncate) the extra tokens.

  Example:
  - If tokenized length = 200, `max_length` = 128:
    - It keeps the first 128 tokens only.

- `return_tensors="pt"`  
  Returns PyTorch tensors instead of Python lists or NumPy arrays.

So `encoding` is a dictionary-like object (a `BatchEncoding`) containing, among other things:

- `encoding["input_ids"]`: shape `[1, max_length]` (batch size = 1 here).
- `encoding["attention_mask"]`: shape `[1, max_length]`.

The batch dimension of 1 is because we passed a single string; the tokenizer always returns batches.

---

### 4.2 Squeeze batch dimension and return dict

```python
    return {
        "input_ids": encoding["input_ids"].squeeze(0),
        "attention_mask": encoding["attention_mask"].squeeze(0)
    }
```

- `encoding["input_ids"]` has shape `[1, max_length]`.
- `.squeeze(0)` removes the batch dimension (dimension 0) of size 1, making it `[max_length]`.

Similarly for `attention_mask`.

So the return structure is:

```python
{
  "input_ids": tensor of shape [max_length],
  "attention_mask": tensor of shape [max_length]
}
```

These are exactly what your `CustomDataset` and `TextModule` expect:

- In `CustomDataset.__getitem__` you do:
  ```python
  text_tensor = tokenize_text(text, max_length=MAX_TEXT_LEN)
  output = {
      "text": text_tensor["input_ids"],
      "attention_mask": text_tensor["attention_mask"],
      ...
  }
  ```

- In `TextModule.forward` you use:
  ```python
  outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
  ```

So the shapes line up perfectly.

---

## Visual summary of the text pipeline

Putting it all together:

1. **Raw text from CSV**  
   e.g. `"  Hello, WORLD!!! Visit http://example.com  "`

2. **clean_text(text)**  
   - Lowercase → `"  hello, world!!! visit http://example.com  "`  
   - Strip → `"hello, world!!! visit http://example.com"`  
   - Remove special chars (keep letters, digits, spaces) → `"hello world visit httpwwwexamplecom"`  
   - Collapse whitespace → `"hello world visit httpwwwexamplecom"`

3. **tokenize_text(cleaned_text, max_length=128)**  
   - Add `[CLS]` and `[SEP]`
   - Split into BERT subword tokens
   - Map each token to an ID → `input_ids` `[101, 7592, 2088, ... , 102, 0, 0, ..., 0]`
   - Create `attention_mask` → `[1, 1, 1, ..., 1, 0, 0, ..., 0]`
   - Truncate if longer than 128, pad if shorter
   - Return tensors of shape `[128]`.

4. **Into the model**  
   - `input_ids` and `attention_mask` go into BERT (`TextModule`).
   - BERT outputs a vector representation, used by the multimodal model.

Diagram:

```text
Raw Text
   │
   ▼
clean_text
   │  (lowercase, remove special chars, fix spaces)
   ▼
Cleaned Text
   │
   ▼
tokenize_text (BERT tokenizer)
   │  (special tokens, tokenization, pad/truncate, to tensors)
   ▼
{
  "input_ids": [max_length],
  "attention_mask": [max_length]
}
   │
   ▼
BERT (in TextModule)
   │
   ▼
Text embedding vector
```

If you want, I can next show a small concrete example with an input string and what `clean_text` and `tokenize_text` produce numerically (at a high level).